{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Wellcome to My Blog \u00b6","title":"Welcome"},{"location":"#wellcome-to-my-blog","text":"","title":"Wellcome to My Blog"},{"location":"Blog/","text":"Blog Posts \u00b6 Software Enginnering 5 Risks That Should Be Avoided in Software Projects Automating Release cycle activities Automate Unit Tests Coverage Reports and Dependency Updates for Your Repositories. Tips for Implementing a Software Release Process Avoid Production Incidents by Considering Production Conditions During Development Improve Code Quality and Git History with Automation tools How to Add Badges to a GitHub Repository How to Use Environment Variables with RSpec Unit Tests Build a Health Check Rack Middleware Kubernetes How to Migrate From Docker Compose to Kubernetes Resource Limits on Kubernetes Helm Chart Wait for All Dependencies Before Starting Kubernetes Pods Docker Swarm Six Tips For Running Swarm Cluster in Production Portainer Review How to Limit Resources for Docker Swarm Services Zero Downtime Deployments With Docker Swarm How to Deploy Portus and Docker Registry in Swarm Orchestrating a Rails Docker Deployment in Swarm Traefik Traefik 2 High Available Mode Prevent DDoS Attacks with Traefik 2 Integrate Traefik 2.1 Reverse Proxy with Docker Swarm Services Monitoring Traefik With Grafana Traefik 2 and Open Tracing Applicaion Logs && Centralized logging Parse Ruby on Rails logs with FluentD Centralize Your Docker Logging With FluentD How to Correlate Rails Requests to Database Logs Centralize Your Docker Logging With Syslog Collect and Store Rails Console Logs Ruby on Rails Single Line Logging Seven Tips For Using Fluentd For Logs Collection Monitoring Monitor Delayed Jobs with Prometheus and Grafana 2 Monitor Delayed Jobs with Prometheus and Grafana 1 Containers and Orchestrators Five Use Cases For Docker Entry Points Cache and Serve Rails Static Assets With Nginx Reverse Proxy Three Methods to Share Rails Assets With Nginx How to Detect Vulnerabilities in Docker Images Dockerizing Rails Applications Part 3: CI/CD Integration Dockerizing Rails Applications Part 2: Automation Dockerizing Rails Applications Part 1: Writing the Dockerfile Automate Docker Registry Cleanup How to Build Docker Base Images for Rails DevOps Tools && Tricks Build GitHub Actions Using Docker Containers Build a Blog With GitHub and MkDocs 5 Tips for Hosting Your Own Jenkins Instance How to Minimize the Costs of Running AWS EC2 Instances Using Terraform Build an OpenLDAP Docker Image That\u2019s Populated With Users Create SonarQube Jenkins webhook How to Add Github Webhooks to a Jenkins Pipeline Improve Your Productivity Using Git and Bash Aliases Host a Secure Private Gem Server With LDAP Authentication and Authorization","title":"Blog Posts"},{"location":"Blog/#blog-posts","text":"Software Enginnering 5 Risks That Should Be Avoided in Software Projects Automating Release cycle activities Automate Unit Tests Coverage Reports and Dependency Updates for Your Repositories. Tips for Implementing a Software Release Process Avoid Production Incidents by Considering Production Conditions During Development Improve Code Quality and Git History with Automation tools How to Add Badges to a GitHub Repository How to Use Environment Variables with RSpec Unit Tests Build a Health Check Rack Middleware Kubernetes How to Migrate From Docker Compose to Kubernetes Resource Limits on Kubernetes Helm Chart Wait for All Dependencies Before Starting Kubernetes Pods Docker Swarm Six Tips For Running Swarm Cluster in Production Portainer Review How to Limit Resources for Docker Swarm Services Zero Downtime Deployments With Docker Swarm How to Deploy Portus and Docker Registry in Swarm Orchestrating a Rails Docker Deployment in Swarm Traefik Traefik 2 High Available Mode Prevent DDoS Attacks with Traefik 2 Integrate Traefik 2.1 Reverse Proxy with Docker Swarm Services Monitoring Traefik With Grafana Traefik 2 and Open Tracing Applicaion Logs && Centralized logging Parse Ruby on Rails logs with FluentD Centralize Your Docker Logging With FluentD How to Correlate Rails Requests to Database Logs Centralize Your Docker Logging With Syslog Collect and Store Rails Console Logs Ruby on Rails Single Line Logging Seven Tips For Using Fluentd For Logs Collection Monitoring Monitor Delayed Jobs with Prometheus and Grafana 2 Monitor Delayed Jobs with Prometheus and Grafana 1 Containers and Orchestrators Five Use Cases For Docker Entry Points Cache and Serve Rails Static Assets With Nginx Reverse Proxy Three Methods to Share Rails Assets With Nginx How to Detect Vulnerabilities in Docker Images Dockerizing Rails Applications Part 3: CI/CD Integration Dockerizing Rails Applications Part 2: Automation Dockerizing Rails Applications Part 1: Writing the Dockerfile Automate Docker Registry Cleanup How to Build Docker Base Images for Rails DevOps Tools && Tricks Build GitHub Actions Using Docker Containers Build a Blog With GitHub and MkDocs 5 Tips for Hosting Your Own Jenkins Instance How to Minimize the Costs of Running AWS EC2 Instances Using Terraform Build an OpenLDAP Docker Image That\u2019s Populated With Users Create SonarQube Jenkins webhook How to Add Github Webhooks to a Jenkins Pipeline Improve Your Productivity Using Git and Bash Aliases Host a Secure Private Gem Server With LDAP Authentication and Authorization","title":"Blog Posts"},{"location":"actions/Docker-Deployment/","text":"Docker Deployment Action \u00b6 A GitHub Action that supports docker-compose and Docker Swarm deployments. Examples \u00b6 Below is a brief examples on how the action can be used: Swarm - name: Deploy to Docker swarm uses: wshihadeh/docker-deployment-action@v1 with: remote_docker_host: user@myswarm.com ssh_private_key: ${{ secrets.DOCKER_SSH_PRIVATE_KEY }} ssh_public_key: ${{ secrets.DOCKER_SSH_PUBLIC_KEY }} deployment_mode: docker-swarm args: my_applicaion Compose - name: Deploy to Docker Host uses: wshihadeh/docker-deployment-action@v1 with: remote_docker_host: user@myswarm.com ssh_private_key: ${{ secrets.DOCKER_SSH_PRIVATE_KEY }} ssh_public_key: ${{ secrets.DOCKER_SSH_PUBLIC_KEY }} deployment_mode: docker-compose args: up -d pre_deployment_command_args: 'bundle exec rake db:migrate' docker_prune: 'true' pull_images_first: 'true' Compose with copy - name: Deploy to Docker Host uses: wshihadeh/docker-deployment-action@v1 with: remote_docker_host: user@myswarm.com ssh_private_key: ${{ secrets.DOCKER_SSH_PRIVATE_KEY }} ssh_public_key: ${{ secrets.DOCKER_SSH_PUBLIC_KEY }} deployment_mode: docker-compose copy_stack_file: true deploy_path: /root/my-deployment stack_file_name: docker-compose.yaml keep_files: 5 args: up -d docker_prune: 'false' pull_images_first: 'false' Swarm with copy - name: Deploy to Docker swarm uses: wshihadeh/docker-deployment-action@v1 with: remote_docker_host: user@myswarm.com ssh_private_key: ${{ secrets.DOCKER_SSH_PRIVATE_KEY }} ssh_public_key: ${{ secrets.DOCKER_SSH_PUBLIC_KEY }} deployment_mode: docker-swarm copy_stack_file: true deploy_path: /root/my-deployment stack_file_name: docker-stack.yaml keep_files: 5 args: my_applicaion Input Configurations \u00b6 Below are all of the supported inputs. Some inputs are considered sensitive information and it should be stored as secrets. args \u00b6 Arguments to pass to the deployment command either docker or docker-compose . The actions will automatically generate the follwing commands for each of the cases. docker stack deploy --compose-file $FILE --log-level debug --host $HOST docker-compose -f $INPUT_STACK_FILE_NAME remote_docker_host \u00b6 Specify Remote Docker host. The input value must be in the follwing format (user@host) ssh_public_key \u00b6 Remote Docker SSH public key. ssh_private_key \u00b6 SSH private key used to connect to the docker host deployment_mode \u00b6 Deployment mode either docker-swarm or docker-compose. Default is docker-compose. copy_stack_file \u00b6 Copy stack file to remote server and deploy from the server. Default is false. deploy_path \u00b6 The path where the stack files will be copied to. Default ~/docker-deployment. stack_file_name \u00b6 Docker stack file used. Default is docker-compose.yaml keep_files \u00b6 Number of the files to be kept on the server. Default is 3. docker_prune \u00b6 A boolean input to trigger docker prune command. pre_deployment_command_args \u00b6 The args for the pre deploument command. Applicable only for docker-compose. pull_images_first \u00b6 Pull docker images before deploying. Applicable only for docker-compose. License \u00b6 This project is licensed under the MIT license. See the LICENSE file for details.","title":"Docker Deployment"},{"location":"actions/Docker-Deployment/#docker-deployment-action","text":"A GitHub Action that supports docker-compose and Docker Swarm deployments.","title":"Docker Deployment Action"},{"location":"actions/Docker-Deployment/#examples","text":"Below is a brief examples on how the action can be used: Swarm - name: Deploy to Docker swarm uses: wshihadeh/docker-deployment-action@v1 with: remote_docker_host: user@myswarm.com ssh_private_key: ${{ secrets.DOCKER_SSH_PRIVATE_KEY }} ssh_public_key: ${{ secrets.DOCKER_SSH_PUBLIC_KEY }} deployment_mode: docker-swarm args: my_applicaion Compose - name: Deploy to Docker Host uses: wshihadeh/docker-deployment-action@v1 with: remote_docker_host: user@myswarm.com ssh_private_key: ${{ secrets.DOCKER_SSH_PRIVATE_KEY }} ssh_public_key: ${{ secrets.DOCKER_SSH_PUBLIC_KEY }} deployment_mode: docker-compose args: up -d pre_deployment_command_args: 'bundle exec rake db:migrate' docker_prune: 'true' pull_images_first: 'true' Compose with copy - name: Deploy to Docker Host uses: wshihadeh/docker-deployment-action@v1 with: remote_docker_host: user@myswarm.com ssh_private_key: ${{ secrets.DOCKER_SSH_PRIVATE_KEY }} ssh_public_key: ${{ secrets.DOCKER_SSH_PUBLIC_KEY }} deployment_mode: docker-compose copy_stack_file: true deploy_path: /root/my-deployment stack_file_name: docker-compose.yaml keep_files: 5 args: up -d docker_prune: 'false' pull_images_first: 'false' Swarm with copy - name: Deploy to Docker swarm uses: wshihadeh/docker-deployment-action@v1 with: remote_docker_host: user@myswarm.com ssh_private_key: ${{ secrets.DOCKER_SSH_PRIVATE_KEY }} ssh_public_key: ${{ secrets.DOCKER_SSH_PUBLIC_KEY }} deployment_mode: docker-swarm copy_stack_file: true deploy_path: /root/my-deployment stack_file_name: docker-stack.yaml keep_files: 5 args: my_applicaion","title":"Examples"},{"location":"actions/Docker-Deployment/#input-configurations","text":"Below are all of the supported inputs. Some inputs are considered sensitive information and it should be stored as secrets.","title":"Input Configurations"},{"location":"actions/Docker-Deployment/#args","text":"Arguments to pass to the deployment command either docker or docker-compose . The actions will automatically generate the follwing commands for each of the cases. docker stack deploy --compose-file $FILE --log-level debug --host $HOST docker-compose -f $INPUT_STACK_FILE_NAME","title":"args"},{"location":"actions/Docker-Deployment/#remote_docker_host","text":"Specify Remote Docker host. The input value must be in the follwing format (user@host)","title":"remote_docker_host"},{"location":"actions/Docker-Deployment/#ssh_public_key","text":"Remote Docker SSH public key.","title":"ssh_public_key"},{"location":"actions/Docker-Deployment/#ssh_private_key","text":"SSH private key used to connect to the docker host","title":"ssh_private_key"},{"location":"actions/Docker-Deployment/#deployment_mode","text":"Deployment mode either docker-swarm or docker-compose. Default is docker-compose.","title":"deployment_mode"},{"location":"actions/Docker-Deployment/#copy_stack_file","text":"Copy stack file to remote server and deploy from the server. Default is false.","title":"copy_stack_file"},{"location":"actions/Docker-Deployment/#deploy_path","text":"The path where the stack files will be copied to. Default ~/docker-deployment.","title":"deploy_path"},{"location":"actions/Docker-Deployment/#stack_file_name","text":"Docker stack file used. Default is docker-compose.yaml","title":"stack_file_name"},{"location":"actions/Docker-Deployment/#keep_files","text":"Number of the files to be kept on the server. Default is 3.","title":"keep_files"},{"location":"actions/Docker-Deployment/#docker_prune","text":"A boolean input to trigger docker prune command.","title":"docker_prune"},{"location":"actions/Docker-Deployment/#pre_deployment_command_args","text":"The args for the pre deploument command. Applicable only for docker-compose.","title":"pre_deployment_command_args"},{"location":"actions/Docker-Deployment/#pull_images_first","text":"Pull docker images before deploying. Applicable only for docker-compose.","title":"pull_images_first"},{"location":"actions/Docker-Deployment/#license","text":"This project is licensed under the MIT license. See the LICENSE file for details.","title":"License"},{"location":"ruby-gems/DeepHealthCheck/","text":"Deep Health Check \u00b6 A simple Health status API Getting Started \u00b6 Install gem # Gemfile gem 'deep_health_check' Add middleware before Rails::Rack::Logger to prevent false positive response if other middleware fails when database is down for example ActiveRecord::QueryCache # config/application.rb config.middleware.insert_after \"Rails::Rack::Logger\", DeepHealthCheck::MiddlewareHealthCheck Protect Health Check endpoints using htauth credentials. \u00b6 Health check middleware expose the following endpoints /health /db_health /tcp_dependencies_health dependencies must follow the below format and can be coonfigured using envioronment varables such as: # TCP_DEPENDENCY_${00..99}=${ip_or_host}:${port} TCP_DEPENDENCY_00=127.0.0.0:8080 /http_dependencies_health dependencies must be valid http url and can be coonfigured using envioronment varables such as: # HTTP_DEPENDENCY_${00..99}=${ip_or_host}:${port} HTTP_DEPENDENCY_00=http://127.0.0.0:8080/health Some of these endpoints provide information about the database and system status. By Default these endpoints are not protected and are accessible publicly. To reduce the security risk introduced by exposing these endpoints, We can protect them using htauth credentials. The following page provide all the necessary steps needed to achieve this task.","title":"DeepHealthCheck"},{"location":"ruby-gems/DeepHealthCheck/#deep-health-check","text":"A simple Health status API","title":"Deep Health Check"},{"location":"ruby-gems/DeepHealthCheck/#getting-started","text":"Install gem # Gemfile gem 'deep_health_check' Add middleware before Rails::Rack::Logger to prevent false positive response if other middleware fails when database is down for example ActiveRecord::QueryCache # config/application.rb config.middleware.insert_after \"Rails::Rack::Logger\", DeepHealthCheck::MiddlewareHealthCheck","title":"Getting Started"},{"location":"ruby-gems/DeepHealthCheck/#protect-health-check-endpoints-using-htauth-credentials","text":"Health check middleware expose the following endpoints /health /db_health /tcp_dependencies_health dependencies must follow the below format and can be coonfigured using envioronment varables such as: # TCP_DEPENDENCY_${00..99}=${ip_or_host}:${port} TCP_DEPENDENCY_00=127.0.0.0:8080 /http_dependencies_health dependencies must be valid http url and can be coonfigured using envioronment varables such as: # HTTP_DEPENDENCY_${00..99}=${ip_or_host}:${port} HTTP_DEPENDENCY_00=http://127.0.0.0:8080/health Some of these endpoints provide information about the database and system status. By Default these endpoints are not protected and are accessible publicly. To reduce the security risk introduced by exposing these endpoints, We can protect them using htauth credentials. The following page provide all the necessary steps needed to achieve this task.","title":"Protect Health Check endpoints using htauth credentials."},{"location":"ruby-gems/DelayedJobMetrics/","text":"DelayedJobMetrics \u00b6 Expose Prometheus Metrics for Delayed Jobs. Installation \u00b6 Gemfile content Add this line to your application's Gemfile: gem 'delayed_job_metrics' And then execute: $~> bundle install Or install it yourself as: $~> gem install delayed_job_metrics Usage & Configurations \u00b6 Enable metrics middelware DELAYED_JOB_METRICS_ENABLED=true Start Rails server and start scraping metrics $~> curl -fs http://127.0.0.1:3000/metrics Overwrite the default metrics endpoint DELAYED_JOB_METRICS_ENNDPOINT=/my_endpoint Setup basic auth Basic Auth Configurations Use the below environment variables to setup the basic authentication for the metrics endpiint HTAUTH_METRICS_USER=user HTAUTH_METRICS_PASSWORD=secret","title":"DelayedJob Metrics"},{"location":"ruby-gems/DelayedJobMetrics/#delayedjobmetrics","text":"Expose Prometheus Metrics for Delayed Jobs.","title":"DelayedJobMetrics"},{"location":"ruby-gems/DelayedJobMetrics/#installation","text":"Gemfile content Add this line to your application's Gemfile: gem 'delayed_job_metrics' And then execute: $~> bundle install Or install it yourself as: $~> gem install delayed_job_metrics","title":"Installation"},{"location":"ruby-gems/DelayedJobMetrics/#usage-configurations","text":"Enable metrics middelware DELAYED_JOB_METRICS_ENABLED=true Start Rails server and start scraping metrics $~> curl -fs http://127.0.0.1:3000/metrics Overwrite the default metrics endpoint DELAYED_JOB_METRICS_ENNDPOINT=/my_endpoint Setup basic auth Basic Auth Configurations Use the below environment variables to setup the basic authentication for the metrics endpiint HTAUTH_METRICS_USER=user HTAUTH_METRICS_PASSWORD=secret","title":"Usage &amp; Configurations"},{"location":"ruby-gems/DockerMetadata/","text":"","title":"Docker Metadata"},{"location":"ruby-gems/EmptyKeys/","text":"","title":"Empty Keys"},{"location":"ruby-gems/IRBTracker/","text":"IRBTracker \u00b6 IRBTracker is a tool that helps in tracking and correlating all commands exected in the IRB console. the gem supports the following features. Authenticate IRB console users using LDAP. Collect IRB command logs into a local file. Forward Logs to a centralized server using fluent-logger. Installation \u00b6 Gemfile content Add this line to your application's Gemfile: gem 'irb_tracker' And then execute: $~> bundle install Or install it yourself as: $~> gem install irb_tracker Rails Console Usage \u00b6 In your bin/rails file you should add following #!/usr/bin/env ruby APP_PATH = File.expand_path('../config/application', __dir__) require_relative '../config/boot' require 'irb_tracker' require 'irb' require 'io/console' RAILS_USER = STDIN.getpass(\"Email:\") if (IRBTracker::LDAPLogin.authenticate(RAILS_USER, STDIN.getpass(\"Password:\"))) IRB::Context.prepend(IRBTracker::IRBLoggable.new(\"MyRailsApp\")) require 'rails/commands' else puts \"Invalid email and user\" end Configurations \u00b6 Below is a list of environment variables used by the gem LDAP Environment Variables LDAP_HOST LDAP_PORT LDAP_BASE LDAP_ADMIN_USER LDAP_ADMIN_PASSWORD FluentD Environment Variables FLUENTD_LOGGING_ENABLED FLUENTD_LOG_HOST FLUENTD_LOG_HOST_PORT","title":"IRBTracker"},{"location":"ruby-gems/IRBTracker/#irbtracker","text":"IRBTracker is a tool that helps in tracking and correlating all commands exected in the IRB console. the gem supports the following features. Authenticate IRB console users using LDAP. Collect IRB command logs into a local file. Forward Logs to a centralized server using fluent-logger.","title":"IRBTracker"},{"location":"ruby-gems/IRBTracker/#installation","text":"Gemfile content Add this line to your application's Gemfile: gem 'irb_tracker' And then execute: $~> bundle install Or install it yourself as: $~> gem install irb_tracker","title":"Installation"},{"location":"ruby-gems/IRBTracker/#rails-console-usage","text":"In your bin/rails file you should add following #!/usr/bin/env ruby APP_PATH = File.expand_path('../config/application', __dir__) require_relative '../config/boot' require 'irb_tracker' require 'irb' require 'io/console' RAILS_USER = STDIN.getpass(\"Email:\") if (IRBTracker::LDAPLogin.authenticate(RAILS_USER, STDIN.getpass(\"Password:\"))) IRB::Context.prepend(IRBTracker::IRBLoggable.new(\"MyRailsApp\")) require 'rails/commands' else puts \"Invalid email and user\" end","title":"Rails Console Usage"},{"location":"ruby-gems/IRBTracker/#configurations","text":"Below is a list of environment variables used by the gem LDAP Environment Variables LDAP_HOST LDAP_PORT LDAP_BASE LDAP_ADMIN_USER LDAP_ADMIN_PASSWORD FluentD Environment Variables FLUENTD_LOGGING_ENABLED FLUENTD_LOG_HOST FLUENTD_LOG_HOST_PORT","title":"Configurations"},{"location":"ruby-gems/Key-ValueParser/","text":"FluentD key-Value Parser \u00b6 A FluentD filter plugin to parse FluentD events that folollow key-value format messages and extract attributes defined in the messages. Installation \u00b6 Gemfile content Add this line to your application's Gemfile: gem 'fluent-plugin-filter-kv-parser' And then execute: $~> bundle install Or install it yourself as: $~> gem install fluent-plugin-filter-kv-parser Configurations \u00b6 Configuration Ite Description key The key that contains the message to be parsed. The default value is log remove_key A boolean value to remove the message key after parsing. Default is false filter_out_lines_without_keys A boolean value that indicates whether to remove or keep events with no key-value items. Default is false use_regex A boolean value to indicate whether to use regex for parsing the messages or not. The default is false. remove_prefix A regex to remove a prefix of the message and to exclude it from being parsed keys_delimiter The Key delimiter character (how the keys are separated). The default is space. kv_delimiter_chart The Key-Value delimiter. The default is = filtered_keys List of keys that will be whitelisted filtered_keys_regex A regex to white list extracted keys filtered_keys_delimiter The character used to separate keys defined in filtered_keys Configuration Examples \u00b6 Filter log message and whitelist a specific keys. Configuration <match pattern> @type key_value_parser key log remove_key true remove_prefix /^[^ ]+\\s[^ ]+/ use_regex true filtered_keys key,gkeyn,nkey,skey,akey,zkey </match> Input {'time' => '2013-02-12 22:01:15 UTC', 'log' => \"Start Request key=10 gkey=100 nkey=108 skey='this is a miltispace line' akey=20 zkey=30 dkey=4\"} Output {\"time\"=>\"2013-02-12 22:01:15 UTC\", \"key\"=>\"10\", \"nkey\"=>\"108\", \"skey\"=>\"'this is a miltispace line'\", \"akey\"=>\"20\", \"zkey\"=>\"30\"} Filter log message and whitelist keys that match a given regex. Configuration <match pattern> @type key_value_parser key log remove_key true remove_prefix /^[^ ]+\\s[^ ]+/ use_regex true filtered_keys none filtered_keys_regex /^sub_[a-zA-Z_0-9]+/ </match> Input {'time' => '2013-02-12 22:01:15 UTC', 'log' => \"Start Request sub_key=0 sub_akey=11 sub_zkey=12 key=10 gkey=100 nkey=108 skey='this is a miltispace line' akey=20 zkey=30 dkey=4\"} Output {\"time\"=>\"2013-02-12 22:01:15 UTC\", \"sub_key\"=>\"0\", \"sub_akey\"=>\"11\", \"sub_zkey\"=>\"12\"} Filter log message and whitelist keys that match a given regex or are whitelisted . Configuration <match pattern> @type key_value_parser key log remove_key true remove_prefix /^[^ ]+\\s[^ ]+/ use_regex true filtered_keys key,gkey filtered_keys_regex /^sub_[a-zA-Z_0-9]+/ </match> Input {'time' => '2013-02-12 22:01:15 UTC', 'log' => \"Start Request sub_key=0 sub_akey=11 sub_zkey=12 key=10 gkey=100 nkey=108 skey='this is a miltispace line' akey=20 zkey=30 dkey=4\"} Output {\"time\"=>\"2013-02-12 22:01:15 UTC\", \"gkey\"=>\"100\", \"key\"=>\"10\", \"sub_key\"=>\"0\", \"sub_akey\"=>\"11\", \"sub_zkey\"=>\"12\"}","title":"Key-Value Parser"},{"location":"ruby-gems/Key-ValueParser/#fluentd-key-value-parser","text":"A FluentD filter plugin to parse FluentD events that folollow key-value format messages and extract attributes defined in the messages.","title":"FluentD key-Value Parser"},{"location":"ruby-gems/Key-ValueParser/#installation","text":"Gemfile content Add this line to your application's Gemfile: gem 'fluent-plugin-filter-kv-parser' And then execute: $~> bundle install Or install it yourself as: $~> gem install fluent-plugin-filter-kv-parser","title":"Installation"},{"location":"ruby-gems/Key-ValueParser/#configurations","text":"Configuration Ite Description key The key that contains the message to be parsed. The default value is log remove_key A boolean value to remove the message key after parsing. Default is false filter_out_lines_without_keys A boolean value that indicates whether to remove or keep events with no key-value items. Default is false use_regex A boolean value to indicate whether to use regex for parsing the messages or not. The default is false. remove_prefix A regex to remove a prefix of the message and to exclude it from being parsed keys_delimiter The Key delimiter character (how the keys are separated). The default is space. kv_delimiter_chart The Key-Value delimiter. The default is = filtered_keys List of keys that will be whitelisted filtered_keys_regex A regex to white list extracted keys filtered_keys_delimiter The character used to separate keys defined in filtered_keys","title":"Configurations"},{"location":"ruby-gems/Key-ValueParser/#configuration-examples","text":"Filter log message and whitelist a specific keys. Configuration <match pattern> @type key_value_parser key log remove_key true remove_prefix /^[^ ]+\\s[^ ]+/ use_regex true filtered_keys key,gkeyn,nkey,skey,akey,zkey </match> Input {'time' => '2013-02-12 22:01:15 UTC', 'log' => \"Start Request key=10 gkey=100 nkey=108 skey='this is a miltispace line' akey=20 zkey=30 dkey=4\"} Output {\"time\"=>\"2013-02-12 22:01:15 UTC\", \"key\"=>\"10\", \"nkey\"=>\"108\", \"skey\"=>\"'this is a miltispace line'\", \"akey\"=>\"20\", \"zkey\"=>\"30\"} Filter log message and whitelist keys that match a given regex. Configuration <match pattern> @type key_value_parser key log remove_key true remove_prefix /^[^ ]+\\s[^ ]+/ use_regex true filtered_keys none filtered_keys_regex /^sub_[a-zA-Z_0-9]+/ </match> Input {'time' => '2013-02-12 22:01:15 UTC', 'log' => \"Start Request sub_key=0 sub_akey=11 sub_zkey=12 key=10 gkey=100 nkey=108 skey='this is a miltispace line' akey=20 zkey=30 dkey=4\"} Output {\"time\"=>\"2013-02-12 22:01:15 UTC\", \"sub_key\"=>\"0\", \"sub_akey\"=>\"11\", \"sub_zkey\"=>\"12\"} Filter log message and whitelist keys that match a given regex or are whitelisted . Configuration <match pattern> @type key_value_parser key log remove_key true remove_prefix /^[^ ]+\\s[^ ]+/ use_regex true filtered_keys key,gkey filtered_keys_regex /^sub_[a-zA-Z_0-9]+/ </match> Input {'time' => '2013-02-12 22:01:15 UTC', 'log' => \"Start Request sub_key=0 sub_akey=11 sub_zkey=12 key=10 gkey=100 nkey=108 skey='this is a miltispace line' akey=20 zkey=30 dkey=4\"} Output {\"time\"=>\"2013-02-12 22:01:15 UTC\", \"gkey\"=>\"100\", \"key\"=>\"10\", \"sub_key\"=>\"0\", \"sub_akey\"=>\"11\", \"sub_zkey\"=>\"12\"}","title":"Configuration Examples"},{"location":"ruby-gems/Orca/","text":"Swarm Orca \u00b6 This gem includes a set of capistrano recipes used to deploy services and rails applications to a swarm cluster. Command Line \u00b6 Install \u00b6 $~> gem install swarm_orca Commands \u00b6 $~> orca decrypt KEY CIPHER # This Command will decrypt the given cipher $~> orca encrypt KEY TEXT # This Command will encrypt the given text $~> orca gen_enc_key # This Command will generate new encryption key $~> orca help [COMMAND] # Describe available commands or one specific command $~> orca new ORCA_DIRECTORY_NAME GIT_FORK DOCKER_NETWORK # This Command will create a new Orca project Install \u00b6 Add the following to your Gemfile . group :deployment do gem 'capistrano' gem 'swarm_orca' end Then run $~> bundle install Usage \u00b6 Add the following to your Capfile . require \"swarm_orca\" Add the following to your deploy.rb . require \"capistrano/swarm_orca/set_global_config\" set :fork, \"${YOUR_FORK_NAME}\" # example set (:db_apps_stacks_mapping), { core: %w(api-backend mobile-backedn), frontend: %w(), } require \"capistrano/swarm_orca/deploy\" require \"capistrano/swarm_orca/docker\" Define your services and application \u00b6 Swarm orca define the managed services and applications in deploy.rb . service_stacks: array include the services stack names. That do not need custome docker images and do not have a database. service_stacks_with_build_image: services that need a custome docker image. db_apps_stacks_mapping: stacks for your rails application. keys are the stack names and values are the database application names. For example, the core stack include two database applications. While the frontend have no database application. set (:db_apps_stacks_mapping), { core: %w(api-backend mobile-backedn), frontend: %w(), } elasticsearch_apps: application names that are connected to elasticsearch. Define your services and applications configurations \u00b6 Swarm Orca manage the stacks and application configurations in the stage files under capistrano/config/deploy each stack must have the stack_name to define the stack name. docker image , tag and other configs can be configured in the same way. These items depend on the stack file orca/application_stack it self and the applications needed by the application. set :mysql, stack_name: 'mysql', mysql_docker_image: 'mysql', mysql_docker_image_tag: '5.7', mysql_volume: \"#{fetch(:deploy_to)}/mysql\" Special config keys: {application}_database_url: Database URL for the rails application. docker_image_prefix: docker image prefix(host and namespace) {application}_docker_image: Docker image name {application}_docker_image_tag: Docker image tag. Application Seeds \u00b6 Define your application seeds By default Swarm orca gem will execute rake db:seed to run the seeds for your defined applications. In Addition, Swarm orca gem support custom seeds. To implement a custom seed in your Orca please follow these instruction. Create a new folder in the root directory and call it \"seeds\". Add a seed file for each of your application. The file name should follow this schema \"${application_name}.rb\" Example of seeds for backend application cat seeds/backend.rb Rails.application.load_tasks Rake::Task['roles:reseed_defaults'].execute Application Database migrations \u00b6 Define migrations, seeds and elasticsearch reindex. To be able to execute migrations, seeds and elasticsearch reindex, you need to explicitly define the roles on your stage. Example: our backend stack is using db and elasticsearch server \"${server}\", user: \"deploy\", roles: %w{${stack}_db ${stack}_reindex} server \"${server}\", user: \"deploy\", roles: %w{backend_db backend_reindex} Deployment Strategy \u00b6 By default capistrano supports only git deployment strategy to support xopy strategy do the following: Apply these changes to your Capfile -require \"capistrano/scm/git\" +scm = ENV.fetch('SCM', 'git') +require \"capistrano/scm/#{scm}\" +install_plugin Module.const_get(\"Capistrano::SCM::#{scm.capitalize}\") To deploy with Copy strategy add \"SCM=copy\" to the command line. $~> SCM=copy bundle exec cap ${stage} deploy:${stack} One Time migrations \u00b6 Execute One Time migrations from orca. Orca generate a rake task for executing one time migrations for each of the defined database application. These tasks are defined like the following example. deploy:otm_${application}[${migration_name}] # deploy:otm_backend[update_referral_bonus_transactions_text] The migration task should be defined in the project source code under the name space \"one_time_migrations\" Data migrations \u00b6 To support data migration for individual applications, do the following. - Add a new configuration item with the following pattern \"${app_name}_data_migrate\" to your stack configurations. The value of the item must be true to switch db:migrate to db:migrate:with_data. See the example below for add migrate with data support to a backend application. set :backend, { stack_name: 'backend', backend_data_migrate: 'true', .... } ERB templates \u00b6 Setup Stacks ERB templates Create an ERB template for each of your stacks. Use the extension \"erb\" for your stacks. Example : docker-stack-backend.yml.erb Add the following line to deploy.rb set (:docker_erb_templates) { true } Ensure the all container environment variables are double-qouted Shared Configuration \u00b6 To set shared configurations that are available for all stacks, use the following syntax. set :shared, { global_variable: 'value_123' } To set shared application configurations that are available in all stacks with special key. Need to create shared application set like: set :backend_shared, { environment_label: \"staging\", } Add special key to include shared application config to application config set :web_backend { include_shared_config: 'backend_shared', } set :mobile_backend { include_shared_config: 'backend_shared', } Local Deployment \u00b6 Support Local Deployment with Swarm Orca To Be able to deploy locally with orca, you need to implement the following changes on your orca project. Create a local stage with your configs. See below template. server \"${server}\", ENV.fetch('USER', '${user}'), roles: %w{ swarm_manager } set (:deploy_to) { \"${deploy_to_path}\" } ${server}: Can be replaced by by \"localhost\", \"127.0.0.1\" or any domain that include \"local\" in it. ${user}: Default it will use the ENV 'USER' value, but if this ENV 'USER' is not defined, you can also defined another/own user. ${deploy_to_path}: the destination deploy to path. You mast include at least \"swarm_manager\" role. Example deploying mysql and rabbitMq server \"127.0.0.1\", ENV.fetch('USER', 'shihadeh'), roles: %w{ mysql rabbitmq swarm_manager } set (:deploy_to) { \"/Users/shihadeh/ggs\" } # set the path to the docker command. set (:docker_path) { \"\" } set :mysql, { stack_name: 'mysql', mysql_docker_image: 'mysql', mysql_docker_image_tag: '5.7', } set :rabbitmq, { rabbitmq_docker_image: 'rabbitmq', stack_name: 'rabbitmq_fr', rabbitmq_docker_image_tag: '3.6-management', rabbitmq_volume: '/Users/shihadeh/ggs/rmq' } Encrypted attribute \u00b6 To support encrypted configuration do the follwing: Create a new encryption key using orc cli. Use orca cli to encrypt the attributes. set the encrypted attributes (you must prefix the attribute key with 'encrypted_' ie. encrypted_cs_database_url ). for instance set :backend, stack_name: 'backend', backend_docker_image: 'backend', backend_docker_image_tag: 'develop', encrypted_backend_database_url: 'h/UNau5AFvhxDbUkBZbPw6RBJzkTPjIMmWOQ+lQ==', export the encryption key one the machine where the deployemnt scripts will be executed ie (your local machine or jenkins nodes). Swarm Orca Deployment \u00b6 Start deployment \u00b6 You can use the following commands to setup and deploy locally # setup and deploy, it will created dbs, and run seeds $~> bundle exec cap ${stage} deploy:development_setup Build custom docker images manually $~> bundle exec cap ${stage} deploy:build_images Deploy individual stacks $~> bundle exec cap ${stage} deploy:${stack} Deployment with specific fork Swarm orca defines a defult for for deployment. The fork value can be changed by setting the ENV 'FORK'. $~> FORK=${forkName} bundle exec cap ${stage} deploy:${stack} Example deploying with fork 'shihadeh' $~> FORK=shihadeh bundle exec cap ${stage} deploy:${stack} Deploy more than one stack $~> DEPLOYED_STACKS=\"mysql rabbitmq\" bundle exec cap ${stage} deploy:auto Deploy without building docker images. $~> BUILD_IMAGE=false bundle exec cap ${stage} deploy:${stack} Deploy all $~> bundle exec cap ${stage} deploy:all Recreate DBS for an environment Use deploy:recreate_all_dbs task to return an envornment(stage) databases to the initial stage $~> bundle exec cap ${stage} deploy:recreate_all_dbs Deploy Debug Mode $~> ORCA_DEBUG=true bundle exec cap ${stage} ${task} Deployment without cleaning up old docker images. Add the PRUNE=false variable to your deployment command. $~> PRUNE=false bundle exec cap local deploy:auto Swarm Orca special roles \u00b6 swarm_manager : Nodes with this command will be used to execute deployment commands. You only need one node this role per stage. swarm_node: Swarm Orca will try to cleanup old images or containers on the nodes with this role. stack: To be able to deploy service X to a given cluster, you must to include the X as a role for a manager node in that cluster. #{stack}_db : This role indeicate where the database migration will be executed. #{stack}_reindex : This role indeicate where the elasticsearch reindexing will be executed.","title":"Swarm Orca"},{"location":"ruby-gems/Orca/#swarm-orca","text":"This gem includes a set of capistrano recipes used to deploy services and rails applications to a swarm cluster.","title":"Swarm Orca"},{"location":"ruby-gems/Orca/#command-line","text":"","title":"Command Line"},{"location":"ruby-gems/Orca/#install","text":"$~> gem install swarm_orca","title":"Install"},{"location":"ruby-gems/Orca/#commands","text":"$~> orca decrypt KEY CIPHER # This Command will decrypt the given cipher $~> orca encrypt KEY TEXT # This Command will encrypt the given text $~> orca gen_enc_key # This Command will generate new encryption key $~> orca help [COMMAND] # Describe available commands or one specific command $~> orca new ORCA_DIRECTORY_NAME GIT_FORK DOCKER_NETWORK # This Command will create a new Orca project","title":"Commands"},{"location":"ruby-gems/Orca/#install_1","text":"Add the following to your Gemfile . group :deployment do gem 'capistrano' gem 'swarm_orca' end Then run $~> bundle install","title":"Install"},{"location":"ruby-gems/Orca/#usage","text":"Add the following to your Capfile . require \"swarm_orca\" Add the following to your deploy.rb . require \"capistrano/swarm_orca/set_global_config\" set :fork, \"${YOUR_FORK_NAME}\" # example set (:db_apps_stacks_mapping), { core: %w(api-backend mobile-backedn), frontend: %w(), } require \"capistrano/swarm_orca/deploy\" require \"capistrano/swarm_orca/docker\"","title":"Usage"},{"location":"ruby-gems/Orca/#define-your-services-and-application","text":"Swarm orca define the managed services and applications in deploy.rb . service_stacks: array include the services stack names. That do not need custome docker images and do not have a database. service_stacks_with_build_image: services that need a custome docker image. db_apps_stacks_mapping: stacks for your rails application. keys are the stack names and values are the database application names. For example, the core stack include two database applications. While the frontend have no database application. set (:db_apps_stacks_mapping), { core: %w(api-backend mobile-backedn), frontend: %w(), } elasticsearch_apps: application names that are connected to elasticsearch.","title":"Define your services and application"},{"location":"ruby-gems/Orca/#define-your-services-and-applications-configurations","text":"Swarm Orca manage the stacks and application configurations in the stage files under capistrano/config/deploy each stack must have the stack_name to define the stack name. docker image , tag and other configs can be configured in the same way. These items depend on the stack file orca/application_stack it self and the applications needed by the application. set :mysql, stack_name: 'mysql', mysql_docker_image: 'mysql', mysql_docker_image_tag: '5.7', mysql_volume: \"#{fetch(:deploy_to)}/mysql\" Special config keys: {application}_database_url: Database URL for the rails application. docker_image_prefix: docker image prefix(host and namespace) {application}_docker_image: Docker image name {application}_docker_image_tag: Docker image tag.","title":"Define your services and applications configurations"},{"location":"ruby-gems/Orca/#application-seeds","text":"Define your application seeds By default Swarm orca gem will execute rake db:seed to run the seeds for your defined applications. In Addition, Swarm orca gem support custom seeds. To implement a custom seed in your Orca please follow these instruction. Create a new folder in the root directory and call it \"seeds\". Add a seed file for each of your application. The file name should follow this schema \"${application_name}.rb\" Example of seeds for backend application cat seeds/backend.rb Rails.application.load_tasks Rake::Task['roles:reseed_defaults'].execute","title":"Application Seeds"},{"location":"ruby-gems/Orca/#application-database-migrations","text":"Define migrations, seeds and elasticsearch reindex. To be able to execute migrations, seeds and elasticsearch reindex, you need to explicitly define the roles on your stage. Example: our backend stack is using db and elasticsearch server \"${server}\", user: \"deploy\", roles: %w{${stack}_db ${stack}_reindex} server \"${server}\", user: \"deploy\", roles: %w{backend_db backend_reindex}","title":"Application Database migrations"},{"location":"ruby-gems/Orca/#deployment-strategy","text":"By default capistrano supports only git deployment strategy to support xopy strategy do the following: Apply these changes to your Capfile -require \"capistrano/scm/git\" +scm = ENV.fetch('SCM', 'git') +require \"capistrano/scm/#{scm}\" +install_plugin Module.const_get(\"Capistrano::SCM::#{scm.capitalize}\") To deploy with Copy strategy add \"SCM=copy\" to the command line. $~> SCM=copy bundle exec cap ${stage} deploy:${stack}","title":"Deployment Strategy"},{"location":"ruby-gems/Orca/#one-time-migrations","text":"Execute One Time migrations from orca. Orca generate a rake task for executing one time migrations for each of the defined database application. These tasks are defined like the following example. deploy:otm_${application}[${migration_name}] # deploy:otm_backend[update_referral_bonus_transactions_text] The migration task should be defined in the project source code under the name space \"one_time_migrations\"","title":"One Time migrations"},{"location":"ruby-gems/Orca/#data-migrations","text":"To support data migration for individual applications, do the following. - Add a new configuration item with the following pattern \"${app_name}_data_migrate\" to your stack configurations. The value of the item must be true to switch db:migrate to db:migrate:with_data. See the example below for add migrate with data support to a backend application. set :backend, { stack_name: 'backend', backend_data_migrate: 'true', .... }","title":"Data migrations"},{"location":"ruby-gems/Orca/#erb-templates","text":"Setup Stacks ERB templates Create an ERB template for each of your stacks. Use the extension \"erb\" for your stacks. Example : docker-stack-backend.yml.erb Add the following line to deploy.rb set (:docker_erb_templates) { true } Ensure the all container environment variables are double-qouted","title":"ERB templates"},{"location":"ruby-gems/Orca/#shared-configuration","text":"To set shared configurations that are available for all stacks, use the following syntax. set :shared, { global_variable: 'value_123' } To set shared application configurations that are available in all stacks with special key. Need to create shared application set like: set :backend_shared, { environment_label: \"staging\", } Add special key to include shared application config to application config set :web_backend { include_shared_config: 'backend_shared', } set :mobile_backend { include_shared_config: 'backend_shared', }","title":"Shared Configuration"},{"location":"ruby-gems/Orca/#local-deployment","text":"Support Local Deployment with Swarm Orca To Be able to deploy locally with orca, you need to implement the following changes on your orca project. Create a local stage with your configs. See below template. server \"${server}\", ENV.fetch('USER', '${user}'), roles: %w{ swarm_manager } set (:deploy_to) { \"${deploy_to_path}\" } ${server}: Can be replaced by by \"localhost\", \"127.0.0.1\" or any domain that include \"local\" in it. ${user}: Default it will use the ENV 'USER' value, but if this ENV 'USER' is not defined, you can also defined another/own user. ${deploy_to_path}: the destination deploy to path. You mast include at least \"swarm_manager\" role. Example deploying mysql and rabbitMq server \"127.0.0.1\", ENV.fetch('USER', 'shihadeh'), roles: %w{ mysql rabbitmq swarm_manager } set (:deploy_to) { \"/Users/shihadeh/ggs\" } # set the path to the docker command. set (:docker_path) { \"\" } set :mysql, { stack_name: 'mysql', mysql_docker_image: 'mysql', mysql_docker_image_tag: '5.7', } set :rabbitmq, { rabbitmq_docker_image: 'rabbitmq', stack_name: 'rabbitmq_fr', rabbitmq_docker_image_tag: '3.6-management', rabbitmq_volume: '/Users/shihadeh/ggs/rmq' }","title":"Local Deployment"},{"location":"ruby-gems/Orca/#encrypted-attribute","text":"To support encrypted configuration do the follwing: Create a new encryption key using orc cli. Use orca cli to encrypt the attributes. set the encrypted attributes (you must prefix the attribute key with 'encrypted_' ie. encrypted_cs_database_url ). for instance set :backend, stack_name: 'backend', backend_docker_image: 'backend', backend_docker_image_tag: 'develop', encrypted_backend_database_url: 'h/UNau5AFvhxDbUkBZbPw6RBJzkTPjIMmWOQ+lQ==', export the encryption key one the machine where the deployemnt scripts will be executed ie (your local machine or jenkins nodes).","title":"Encrypted attribute"},{"location":"ruby-gems/Orca/#swarm-orca-deployment","text":"","title":"Swarm Orca Deployment"},{"location":"ruby-gems/Orca/#start-deployment","text":"You can use the following commands to setup and deploy locally # setup and deploy, it will created dbs, and run seeds $~> bundle exec cap ${stage} deploy:development_setup Build custom docker images manually $~> bundle exec cap ${stage} deploy:build_images Deploy individual stacks $~> bundle exec cap ${stage} deploy:${stack} Deployment with specific fork Swarm orca defines a defult for for deployment. The fork value can be changed by setting the ENV 'FORK'. $~> FORK=${forkName} bundle exec cap ${stage} deploy:${stack} Example deploying with fork 'shihadeh' $~> FORK=shihadeh bundle exec cap ${stage} deploy:${stack} Deploy more than one stack $~> DEPLOYED_STACKS=\"mysql rabbitmq\" bundle exec cap ${stage} deploy:auto Deploy without building docker images. $~> BUILD_IMAGE=false bundle exec cap ${stage} deploy:${stack} Deploy all $~> bundle exec cap ${stage} deploy:all Recreate DBS for an environment Use deploy:recreate_all_dbs task to return an envornment(stage) databases to the initial stage $~> bundle exec cap ${stage} deploy:recreate_all_dbs Deploy Debug Mode $~> ORCA_DEBUG=true bundle exec cap ${stage} ${task} Deployment without cleaning up old docker images. Add the PRUNE=false variable to your deployment command. $~> PRUNE=false bundle exec cap local deploy:auto","title":"Start deployment"},{"location":"ruby-gems/Orca/#swarm-orca-special-roles","text":"swarm_manager : Nodes with this command will be used to execute deployment commands. You only need one node this role per stage. swarm_node: Swarm Orca will try to cleanup old images or containers on the nodes with this role. stack: To be able to deploy service X to a given cluster, you must to include the X as a role for a manager node in that cluster. #{stack}_db : This role indeicate where the database migration will be executed. #{stack}_reindex : This role indeicate where the elasticsearch reindexing will be executed.","title":"Swarm Orca special roles"},{"location":"ruby-gems/RabbitMQ/","text":"RabbitMQ \u00b6 ____ _ _ _ _ __ __ ___ ____ _ _ _ | _ \\ __ _| |__ | |__ (_) |_| \\/ |/ _ \\ / ___| (_) ___ _ __ | |_ | |_) / _` | '_ \\| '_ \\| | __| |\\/| | | | | | | | | |/ _ \\ '_ \\| __| | _ < (_| | |_) | |_) | | |_| | | | |_| | | |___| | | __/ | | | |_ |_| \\_\\__,_|_.__/|_.__/|_|\\__|_| |_|\\__\\_\\ \\____|_|_|\\___|_| |_|\\__| Simplifying RabbitMQ for Ruby apps! RabbitMQ Client is a ruby client library for applications dealing with RabbitMQ . It wraps common behaviors needed by publishers and subscribers in an easy and convenient API. It uses both bunny and connection_pool to manage RabbitMQ communications. It is extendable using plugins. Why RabbitMQ Client? Why not bunny and connection_pool directly? Well, gems are just a clients. You still need to write a lot of code to manage proper subscribing and publishing of messages. You need to do error handling, passing request headers to RabbitMQ and maybe logging/instrumenting the message management process. Finally, you also need to consider how to deploy your app and how to start it. With RabbitMQ Client by your side, all this becomes smooth and easy. Installation \u00b6 Add this line to your application's Gemfile: gem 'rabbitmq_client' And then execute: $~> bundle Or install it yourself as: $~> gem install rabbitmq_client Usage \u00b6 RabbitMQ Client can be used to publsih and subscribe RabbitMQ messages. Configurations \u00b6 RabbitMQ Client support the following configurations Configuration Description Default Value rabbitmq_url RabbitMQ server URL amqp://guest:guest@127.0.0.1:5672 logger_configs.logs_format RabbitmqClient logs format. values can be either json or plain plain logger_configs.logs_level RabbitmqClient logs level. values can be one of :debug, :info, :error info logger_configs.logs_filename Logs file name, if nil STOUT will be used nil logger_configs.logger Logger object, if nil STOUT logger will created nil session_params.heartbeat_publisher Heartbeat interval for publisher sessions. 0 means no heartbeat 0 session_params.session_pool Number of sessions with rabbitmq 1 plugins Array of used plugins [] global_store Global Store used to store tags and headers: RequestHeaderMiddleware RequestStore nil whitelist List of whitelisted headers ['x-request-id'.to_sym] Full Configuration Example \u00b6 RabbitmqClient.configure do |config| config.rabbitmq_url = \"${rabbitmq_url}\" config.logger_configs = { logs_format: 'plain', logs_level: :info, logs_filename: nil, logger: nil } config.session_params = { heartbeat_publisher: 0, session_pool: 1 } config.plugins = [] config.global_store = RequestHeaderMiddleware config.whitelist = ['x-request-id'.to_sym] end Append plungins or whitelist items \u00b6 RabbitmqClient.plugins << MYRabbitmqClientPlugin RabbitmqClient.whitelist << :white_listed_header_key Publishing messages to RabbitMQ \u00b6 Publish messages using RabbitmqClient singleton publisher # RabbitmqClient.add_exchange(exchange, type, options) RabbitmqClient.add_exchange('default.rabbitmq_client', :topic, {}) # RabbitmqClient.publish(payload, options) RabbitmqClient.publish({id: 10, name: 'rabbitmq_client'}, { exchange_name: 'default.rabbitmq_client' }) Create a publisher and use it config = { rabbitmq_url: val, exchange_registry: val, session_params: { heartbeat_publisher: val } } publisher = RabbitmqClient::Publisher.new(config) publisher.publish(payload, options) Plugins \u00b6 RabbitmqClient plugins are classes that define a callbacks that can be executed before or after events that occuers during RabbitmqClient lifecycle. Current supprted lifecycle events are: publish: occure when publishing a massage is triggered. Here is an example where we define a plugin to add some headers to message options before publishing the message. class MQTestPlugin < RabbitmqClient::Plugin callbacks do |lifecycle| lifecycle.before(:publish) do |_message, options| options[:headers] = { test_key: 'test'} end end end RabbitmqClient.plugins << MQTestPlugin","title":"RabbitMQ Client"},{"location":"ruby-gems/RabbitMQ/#rabbitmq","text":"____ _ _ _ _ __ __ ___ ____ _ _ _ | _ \\ __ _| |__ | |__ (_) |_| \\/ |/ _ \\ / ___| (_) ___ _ __ | |_ | |_) / _` | '_ \\| '_ \\| | __| |\\/| | | | | | | | | |/ _ \\ '_ \\| __| | _ < (_| | |_) | |_) | | |_| | | | |_| | | |___| | | __/ | | | |_ |_| \\_\\__,_|_.__/|_.__/|_|\\__|_| |_|\\__\\_\\ \\____|_|_|\\___|_| |_|\\__| Simplifying RabbitMQ for Ruby apps! RabbitMQ Client is a ruby client library for applications dealing with RabbitMQ . It wraps common behaviors needed by publishers and subscribers in an easy and convenient API. It uses both bunny and connection_pool to manage RabbitMQ communications. It is extendable using plugins. Why RabbitMQ Client? Why not bunny and connection_pool directly? Well, gems are just a clients. You still need to write a lot of code to manage proper subscribing and publishing of messages. You need to do error handling, passing request headers to RabbitMQ and maybe logging/instrumenting the message management process. Finally, you also need to consider how to deploy your app and how to start it. With RabbitMQ Client by your side, all this becomes smooth and easy.","title":"RabbitMQ"},{"location":"ruby-gems/RabbitMQ/#installation","text":"Add this line to your application's Gemfile: gem 'rabbitmq_client' And then execute: $~> bundle Or install it yourself as: $~> gem install rabbitmq_client","title":"Installation"},{"location":"ruby-gems/RabbitMQ/#usage","text":"RabbitMQ Client can be used to publsih and subscribe RabbitMQ messages.","title":"Usage"},{"location":"ruby-gems/RabbitMQ/#configurations","text":"RabbitMQ Client support the following configurations Configuration Description Default Value rabbitmq_url RabbitMQ server URL amqp://guest:guest@127.0.0.1:5672 logger_configs.logs_format RabbitmqClient logs format. values can be either json or plain plain logger_configs.logs_level RabbitmqClient logs level. values can be one of :debug, :info, :error info logger_configs.logs_filename Logs file name, if nil STOUT will be used nil logger_configs.logger Logger object, if nil STOUT logger will created nil session_params.heartbeat_publisher Heartbeat interval for publisher sessions. 0 means no heartbeat 0 session_params.session_pool Number of sessions with rabbitmq 1 plugins Array of used plugins [] global_store Global Store used to store tags and headers: RequestHeaderMiddleware RequestStore nil whitelist List of whitelisted headers ['x-request-id'.to_sym]","title":"Configurations"},{"location":"ruby-gems/RabbitMQ/#full-configuration-example","text":"RabbitmqClient.configure do |config| config.rabbitmq_url = \"${rabbitmq_url}\" config.logger_configs = { logs_format: 'plain', logs_level: :info, logs_filename: nil, logger: nil } config.session_params = { heartbeat_publisher: 0, session_pool: 1 } config.plugins = [] config.global_store = RequestHeaderMiddleware config.whitelist = ['x-request-id'.to_sym] end","title":"Full Configuration Example"},{"location":"ruby-gems/RabbitMQ/#append-plungins-or-whitelist-items","text":"RabbitmqClient.plugins << MYRabbitmqClientPlugin RabbitmqClient.whitelist << :white_listed_header_key","title":"Append plungins or whitelist items"},{"location":"ruby-gems/RabbitMQ/#publishing-messages-to-rabbitmq","text":"Publish messages using RabbitmqClient singleton publisher # RabbitmqClient.add_exchange(exchange, type, options) RabbitmqClient.add_exchange('default.rabbitmq_client', :topic, {}) # RabbitmqClient.publish(payload, options) RabbitmqClient.publish({id: 10, name: 'rabbitmq_client'}, { exchange_name: 'default.rabbitmq_client' }) Create a publisher and use it config = { rabbitmq_url: val, exchange_registry: val, session_params: { heartbeat_publisher: val } } publisher = RabbitmqClient::Publisher.new(config) publisher.publish(payload, options)","title":"Publishing messages to RabbitMQ"},{"location":"ruby-gems/RabbitMQ/#plugins","text":"RabbitmqClient plugins are classes that define a callbacks that can be executed before or after events that occuers during RabbitmqClient lifecycle. Current supprted lifecycle events are: publish: occure when publishing a massage is triggered. Here is an example where we define a plugin to add some headers to message options before publishing the message. class MQTestPlugin < RabbitmqClient::Plugin callbacks do |lifecycle| lifecycle.before(:publish) do |_message, options| options[:headers] = { test_key: 'test'} end end end RabbitmqClient.plugins << MQTestPlugin","title":"Plugins"},{"location":"ruby-gems/RecordSplitter/","text":"FluentD Record Splitter \u00b6 A Fluentd plugin to split fluentd events into multiple records Installation \u00b6 Gemfile content Add this line to your application's Gemfile: gem 'fluent-plugin-record-splitter' And then execute: $~> bundle install Or install it yourself as: $~> gem install fluent-plugin-record-splitter Configurations \u00b6 Configuration Ite Description tag The output tag for the generated records input_key The target key to be splited output_key The generateed splitted key (if not specified input_key will be used) split_stratgey The strategy used to splited the message should be either lines or regex split_regex Regex to split lines shared_keys List of keys to be shared between all generated records remove_keys List of keys to be removed from all generated records append_new_line Append a new line to the end of the input event remove_new_line Remove the new line form the end of the generated events remove_input_key Remove the key spcified by input_key from the generated events Configuration Examples \u00b6 Split lines based on the new line charachter \\n . Configuration <match pattern> @type record_splitter tag splitted.log input_key message split_stratgey lines append_new_line true remove_new_line true shared_keys [\"akey\"] </match> Input {'akey':'c', 'abkey':'cc', 'message': 'line one\\nlines2' } Output {'akey':'c', 'message': 'line one' } {'akey':'c', 'message': 'lines2' } Split lines based on a defined regex. Configuration <match pattern> @type record_splitter tag splitted.log input_key message split_stratgey regex split_regex /\\d+\\s<\\d+>.+/ remove_keys [\"akey\"] </match> Input {'dkey':'c', 'akey':'c', 'abkey':'cc', 'message': '83 <40>1 2012-11-30T06:45:29+00:00 start app\\n90 <40>1 2012-11-30T06:45:26+00:00 host app web.3 - Starting process' } Output {'dkey':'c', 'abkey':'cc', 'message': '83 <40>1 2012-11-30T06:45:29+00:00 start app' } {'dkey':'c', 'abkey':'cc', 'message': '90 <40>1 2012-11-30T06:45:26+00:00 host app web.3 - Starting process' }","title":"Record Splitter"},{"location":"ruby-gems/RecordSplitter/#fluentd-record-splitter","text":"A Fluentd plugin to split fluentd events into multiple records","title":"FluentD Record Splitter"},{"location":"ruby-gems/RecordSplitter/#installation","text":"Gemfile content Add this line to your application's Gemfile: gem 'fluent-plugin-record-splitter' And then execute: $~> bundle install Or install it yourself as: $~> gem install fluent-plugin-record-splitter","title":"Installation"},{"location":"ruby-gems/RecordSplitter/#configurations","text":"Configuration Ite Description tag The output tag for the generated records input_key The target key to be splited output_key The generateed splitted key (if not specified input_key will be used) split_stratgey The strategy used to splited the message should be either lines or regex split_regex Regex to split lines shared_keys List of keys to be shared between all generated records remove_keys List of keys to be removed from all generated records append_new_line Append a new line to the end of the input event remove_new_line Remove the new line form the end of the generated events remove_input_key Remove the key spcified by input_key from the generated events","title":"Configurations"},{"location":"ruby-gems/RecordSplitter/#configuration-examples","text":"Split lines based on the new line charachter \\n . Configuration <match pattern> @type record_splitter tag splitted.log input_key message split_stratgey lines append_new_line true remove_new_line true shared_keys [\"akey\"] </match> Input {'akey':'c', 'abkey':'cc', 'message': 'line one\\nlines2' } Output {'akey':'c', 'message': 'line one' } {'akey':'c', 'message': 'lines2' } Split lines based on a defined regex. Configuration <match pattern> @type record_splitter tag splitted.log input_key message split_stratgey regex split_regex /\\d+\\s<\\d+>.+/ remove_keys [\"akey\"] </match> Input {'dkey':'c', 'akey':'c', 'abkey':'cc', 'message': '83 <40>1 2012-11-30T06:45:29+00:00 start app\\n90 <40>1 2012-11-30T06:45:26+00:00 host app web.3 - Starting process' } Output {'dkey':'c', 'abkey':'cc', 'message': '83 <40>1 2012-11-30T06:45:29+00:00 start app' } {'dkey':'c', 'abkey':'cc', 'message': '90 <40>1 2012-11-30T06:45:26+00:00 host app web.3 - Starting process' }","title":"Configuration Examples"}]}